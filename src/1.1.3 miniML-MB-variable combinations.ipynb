{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# XGBoost with variable combinations:\n",
    "\n",
    "Trains an XGBoost model for all variable combinations to find the best combinations of temperature and total precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setting up:\n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import re\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from ast import literal_eval\n",
    "import matplotlib\n",
    "\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.stakes_processing import *\n",
    "from scripts.xgb_input import *\n",
    "from scripts.xgb_model import *\n",
    "from scripts.plots_clean import *\n",
    "from scripts.xgb_metrics import *\n",
    "from scripts.xgb_model_varcomb import *\n",
    "\n",
    "from scripts.PDD_model_modules import *\n",
    "from scripts.PDD_model_calibration import *\n",
    "from scripts.PDD_helpers import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed everywhere:\n",
    "seed_all(SEED)\n",
    "print('Seed:', SEED)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.01, 0.2, 0.01),\n",
    "    'n_estimators': np.arange(50, 300, 15),\n",
    "    'max_depth': np.arange(3, 10, 1),\n",
    "}\n",
    "\n",
    "feature_list = [\n",
    "    't2m_Oct', 't2m_Nov', 't2m_Dec', 't2m_Jan', 't2m_Feb', 't2m_Mar',\n",
    "    't2m_Apr', 't2m_May', 't2m_June', 't2m_July', 't2m_Aug', 't2m_Sep',\n",
    "    'tp_Oct', 'tp_Nov', 'tp_Dec', 'tp_Jan', 'tp_Feb', 'tp_Mar', 'tp_Apr',\n",
    "    'tp_May', 'tp_June', 'tp_July', 'tp_Aug', 'tp_Sep'\n",
    "]\n",
    "\n",
    "color_palette = sns.color_palette(\"husl\", 13)\n",
    "colors = np.tile(\"#8CA6D9\", 6)\n",
    "palette_grays = sns.color_palette(colors)\n",
    "\n",
    "INPUT_TYPE = \"MeteoSuisse\"\n",
    "\n",
    "KFOLD = True\n",
    "if KFOLD:\n",
    "    NUM_FOLDS = 5\n",
    "    FOLD = 'kfold'\n",
    "else:\n",
    "    NUM_FOLDS = 1\n",
    "    FOLD = 'single_fold'\n",
    "\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of stakes per glacier and their names\n",
    "glStakesNum, glStakes = get_StakesNum(path_GLAMOS_csv)\n",
    "glStakes_sorted = sorted(glStakesNum.items(), key=lambda x: x[1])\n",
    "\n",
    "# Get total number of stakes\n",
    "num_stakes = 0\n",
    "for (glacier, num) in (glStakes_sorted):\n",
    "    num_stakes += num\n",
    "print('Total number of stakes:', num_stakes)\n",
    "print('Number of stakes per glacier:\\n', glStakes_sorted)\n",
    "\n",
    "# glacier names:\n",
    "glaciers = list(glStakes.keys())\n",
    "# Keep only the glaciers with more than 20 years of measurements\n",
    "glStakes_20years, glStakes_20years_sorted, glStakes_20years_all = getStakesNyears(\n",
    "    glaciers,\n",
    "    glStakes,\n",
    "    path_glacattr,\n",
    "    path_era5_stakes,\n",
    "    input_type=INPUT_TYPE,\n",
    "    N=20)\n",
    "print('After preprocessing:\\n----\\nNumber of glaciers:',\n",
    "      len(glStakes_20years.keys()))\n",
    "# num_stakes = 0\n",
    "# for gl in glStakes_20years.keys():\n",
    "#     num_stakes += len(glStakes_20years[gl])\n",
    "print('Number of stakes:', len(glStakes_20years_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename stakes so that ordered by elevation from P1 to P(X highest):\n",
    "glaciers = list(glStakes_20years.keys())\n",
    "s_end, gl_mb, = {}, {}\n",
    "start_years, end_years = [], []\n",
    "stakes_per_el = {}\n",
    "var = \"b_a_fix\"\n",
    "for g in range(len(glaciers)):\n",
    "    gl = glaciers[g]  # One glacier\n",
    "    height = {}\n",
    "    for stake in glStakes_20years[gl]:\n",
    "        # Get coordinates and time of file for this stake:\n",
    "        fileName = re.split(\".csv\", stake)[0][:-3]\n",
    "        df_stake = read_stake_csv(path_glacattr, stake,\n",
    "                                  COI).sort_values(by=\"date_fix0\")\n",
    "\n",
    "        # remove category 0\n",
    "        df_stake = df_stake[df_stake.vaw_id > 0]\n",
    "\n",
    "        # remove 2021:\n",
    "        df_stake = df_stake[df_stake.date_fix0.dt.year < 2021]\n",
    "\n",
    "        # years:\n",
    "        years = [\n",
    "            df_stake.date_fix0.iloc[i].year\n",
    "            for i in range(len(df_stake.date_fix0))\n",
    "        ]\n",
    "\n",
    "        start_years.append(years[0])\n",
    "        end_years.append(years[-1])\n",
    "\n",
    "        s_end[fileName] = years  # start and end years\n",
    "        gl_mb[fileName] = df_stake[var].values / (\n",
    "            1000)  # MB of stake (change to m w.e.)\n",
    "        height[fileName] = df_stake.height.iloc[0]  # Height of stake\n",
    "\n",
    "    # Sort stakes per elevation\n",
    "    print(height)\n",
    "    stakes_per_el[gl] = list(\n",
    "        pd.Series(height).sort_values(ascending=True).index.values)\n",
    "rename_stakes = {}\n",
    "for gl in stakes_per_el.keys():\n",
    "    for i, stake in enumerate(stakes_per_el[gl]):\n",
    "        rename_stakes[stake] = f\"{GLACIER_CORRECT[gl]}-P{i+1}\"\n",
    "\n",
    "rename_stakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## XGBoost - run multi combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature variables\n",
    "t2m_vars = [\n",
    "    't2m_Oct',\n",
    "    't2m_Nov',\n",
    "    't2m_Dec',\n",
    "    't2m_Jan',\n",
    "    't2m_Feb',\n",
    "    't2m_Mar',\n",
    "    't2m_Apr',\n",
    "    't2m_May',\n",
    "    't2m_June',\n",
    "    't2m_July',\n",
    "    't2m_Aug',\n",
    "    't2m_Sep',\n",
    "]\n",
    "# precipitation variables\n",
    "tp_vars = [\n",
    "    'tp_Oct',\n",
    "    'tp_Nov',\n",
    "    'tp_Dec',\n",
    "    'tp_Jan',\n",
    "    'tp_Feb',\n",
    "    'tp_Mar',\n",
    "    'tp_Apr',\n",
    "    'tp_May',\n",
    "    'tp_June',\n",
    "    'tp_July',\n",
    "    'tp_Aug',\n",
    "    'tp_Sep',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all combinations of months: (powerset so not consecutive)\n",
    "def powerset(original_list):\n",
    "    # The number of subsets is 2^n\n",
    "    num_subsets = 2**len(original_list)\n",
    "\n",
    "    # Create an empty list to hold all the subsets\n",
    "    subsets = []\n",
    "\n",
    "    # Iterate over all possible subsets\n",
    "    for subset_index in range(num_subsets):\n",
    "        # Create an empty list to hold the current subset\n",
    "        subset = []\n",
    "        # Iterate over all elements in the original list\n",
    "        for index in range(len(original_list)):\n",
    "            # Check if index bit is set in subset_index\n",
    "            if (subset_index & (1 << index)) != 0:\n",
    "                # If the bit is set, add the element at this index to the current subset\n",
    "                subset.append(original_list[index])\n",
    "        # Add the current subset to the list of all subsets\n",
    "        if len(subset) > 0:\n",
    "            subsets.append(subset)\n",
    "    return subsets\n",
    "\n",
    "\n",
    "combinations = powerset(t2m_vars)\n",
    "len(combinations), combinations[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all consecutive combinations of length max 6:\n",
    "def consecutive_combinations(iterable, consec):\n",
    "    begin = 0\n",
    "    chunks = len(iterable) + 1 - consec\n",
    "    return [iterable[x + begin:x + consec] for x in range(chunks)]\n",
    "\n",
    "\n",
    "iterable = list(MONTH_VAL.keys())\n",
    "consec_t2m, consec_tp = [], []\n",
    "for i in range(1, 7):\n",
    "    for el in consecutive_combinations(iterable, i):\n",
    "        consec_t2m.append(['t2m_' + MONTH_VAL[j] for j in el])\n",
    "        consec_tp.append(['tp_' + MONTH_VAL[j] for j in el])\n",
    "\n",
    "# combinations for t2m and tp\n",
    "combinations_t2m_tp = list(itertools.product(consec_t2m, consec_tp))\n",
    "len(combinations_t2m_tp), combinations_t2m_tp[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all stakes were already run:\n",
    "path_multi = f'../../data/MB_modeling/XGBoost/ind_stakes/kfold/{INPUT_TYPE}/multi_combi/sum_prec_all/'\n",
    "stakes_processed = [\n",
    "    re.split('_', f)[0] + '_' + re.split('_', f)[1][:-4]\n",
    "    for f in os.listdir(path_multi)\n",
    "]\n",
    "remaining_stakes = Diff(list(stakes_processed), list(glStakes_20years_all))\n",
    "remaining_stakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../../data/MB_modeling/XGBoost/ind_stakes/kfold/MeteoSuisse/monthly/t2m_tp/match_annual/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we're not running the models with grid search,\n",
    "# we're getting the best hp from the standard XGBoost run (with all 12 months)\n",
    "var_xg_monthly, metrics_monthly = assembleXGStakes(\n",
    "    path_save_xgboost_stakes +\n",
    "    f'{FOLD}/{INPUT_TYPE}/monthly/t2m_tp/match_annual/', glStakes_20years_all, rename_stakes)\n",
    "hp_lr = metrics_monthly['hp_lr']\n",
    "hp_ne = metrics_monthly['hp_ne']\n",
    "hp_md = metrics_monthly['hp_md']\n",
    "\n",
    "# Run XGBoost for all combinations of t2m variables and save the 20 bests\n",
    "RUN = False\n",
    "path_multi = '../../data/MB_modeling/XGBoost/ind_stakes/kfold/MeteoSuisse/multi_combi/sum_prec_all_mae/'\n",
    "if RUN:\n",
    "    runXGBoost_varcomb(\n",
    "        combinations_t2m_tp,\n",
    "        hp_lr,\n",
    "        hp_ne,\n",
    "        hp_md,\n",
    "        glStakes_20years,\n",
    "        param_grid,  # grid for HP search\n",
    "        path_multi,\n",
    "        mb_match='annual',\n",
    "        input_type=INPUT_TYPE,\n",
    "        log=False,\n",
    "        empty_folder=False,\n",
    "        tp_sum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Analyse best combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "allStakes = []\n",
    "for gl in glStakes_20years.keys():\n",
    "    allStakes.append(glStakes_20years[gl])\n",
    "allStakes = np.concatenate(allStakes)\n",
    "allStakes = [\n",
    "    re.split('_', f)[0] + '_' + re.split('_', f)[1] for f in allStakes\n",
    "]\n",
    "\n",
    "# Check if all stakes were processed:\n",
    "path_multi = path_save_xgboost_stakes + f'{FOLD}/{INPUT_TYPE}/multi_combi/sum_prec_all_mae/'\n",
    "stakes_processed = [\n",
    "    re.split('_', f)[0] + '_' + re.split('_', f)[1][:-4]\n",
    "    for f in os.listdir(path_multi)\n",
    "]\n",
    "remaining_stakes = Diff(list(stakes_processed), list(allStakes))\n",
    "glProcessed = {}\n",
    "for stake in stakes_processed:\n",
    "    glacier = re.split('_', stake)[0]\n",
    "    updateDic(glProcessed, glacier, stake + '_mb.csv')\n",
    "\n",
    "glProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a hash to each combination of t2m and tp:\n",
    "stakes, glacier, rmse, rmse_val, rmse_train, t2m, tp = [], [], [], [], [], [], []\n",
    "mae, mae_val, mae_train = [], [], []\n",
    "for gl in glProcessed.keys():\n",
    "    for stakeNb in range(len(glProcessed[gl])):\n",
    "        # read multicombi\n",
    "        stake = glProcessed[gl][stakeNb]\n",
    "        stakeName = re.split(\".csv\", stake)[0][:-3]\n",
    "        var_df = pd.read_csv(path_multi + f'{stakeName}.csv',\n",
    "                             converters={\n",
    "                                 \"t2m\": literal_eval,\n",
    "                                 \"tp\": literal_eval\n",
    "                             })\n",
    "        if 'test_rmse' in var_df.columns:\n",
    "            rmse.append(var_df['test_rmse'])\n",
    "            rmse_val.append(var_df['val_rmse'])\n",
    "            rmse_train.append(var_df['train_rmse'])\n",
    "            mae.append(var_df['test_mae'])\n",
    "            mae_val.append(var_df['val_mae'])\n",
    "            mae_train.append(var_df['train_mae'])\n",
    "            N_combi = len(var_df['test_rmse'])\n",
    "\n",
    "\n",
    "        stakes.append(np.tile(stakeName, N_combi))\n",
    "        glacier.append(np.tile(gl, N_combi))\n",
    "        t2m.append(var_df['t2m'])\n",
    "        tp.append(var_df['tp'])\n",
    "        \n",
    "dfAllStakes = pd.DataFrame({\n",
    "    'glaciers': np.concatenate(glacier),\n",
    "    'stakes': np.concatenate(stakes),\n",
    "    'test_rmse': np.concatenate(rmse)/(1000),\n",
    "    'val_rmse': np.concatenate(rmse_val)/(1000),\n",
    "    'train_rmse': np.concatenate(rmse_train)/(1000),\n",
    "    'test_mae': np.concatenate(mae)/(1000),\n",
    "    'val_mae': np.concatenate(mae_val)/(1000),\n",
    "    'train_mae': np.concatenate(mae_train)/(1000),\n",
    "    't2m': np.concatenate(t2m),\n",
    "    'tp': np.concatenate(tp)\n",
    "})\n",
    "dfAllStakes['t2m-tp-hash'] = [\n",
    "    makeCombNum(dfAllStakes['t2m'].iloc[i], dfAllStakes['tp'].iloc[i])\n",
    "    for i in range(len(dfAllStakes))\n",
    "]\n",
    "print('Number of unique hashes:', len(dfAllStakes['t2m-tp-hash'].unique()))\n",
    "dfAllStakes.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Best for individual stakes:\n",
    "#### Fifty best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = 'mae'\n",
    "VAL_METRIC = f'val_{METRIC}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importdf_50best, dfWeights_50best = NBestCombinations(dfAllStakes,\n",
    "                                                              INVERSE_MONTH_POS,\n",
    "                                                              t2m_vars,\n",
    "                                                              tp_vars,\n",
    "                                                              N=50,\n",
    "                                                              type=VAL_METRIC)\n",
    "\n",
    "# Aggregate over all stakes:\n",
    "dfWeights_Mean_50best = dfWeights_50best.groupby(['feature', 'month'\n",
    "                                                  ]).sum().reset_index()\n",
    "dfWeights_Mean_50best['freq_var'] = dfWeights_Mean_50best['weight'] / (50 * 30)\n",
    "dfWeights_Mean_50best['type'] = np.tile('50 best', len(dfWeights_Mean_50best))\n",
    "dfWeights_Mean_50best.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeights_50best['stakes_new'] = dfWeights_50best['stakes'].apply(lambda x: rename_stakes[x]) \n",
    "dfWeights_50best['freq_var'] = dfWeights_50best['weight'] / 50\n",
    "g = sns.FacetGrid(\n",
    "    dfWeights_50best,\n",
    "    col=\"stakes_new\",\n",
    "    col_wrap=6,\n",
    "    hue=\"feature\",\n",
    ")\n",
    "g.map(sns.barplot,\n",
    "      \"month\",\n",
    "      \"freq_var\",\n",
    "      orient='v',\n",
    "      order=INVERSE_MONTH_POS.keys(),\n",
    "      alpha=0.5)\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    ax.set_title(col_val)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeights_50best['stakes_new'] = dfWeights_50best['stakes'].apply(lambda x: rename_stakes[x]) \n",
    "dfWeights_50best['freq_var'] = dfWeights_50best['weight'] / 50\n",
    "dfWeights_50best_subset = dfWeights_50best[dfWeights_50best.stakes_new.apply(lambda x: 'Basodino' in x)].sort_values(by = 'stakes_new')\n",
    "g = sns.FacetGrid(\n",
    "    dfWeights_50best_subset,\n",
    "    col=\"stakes_new\",\n",
    "    col_wrap=3,\n",
    "    hue=\"feature\",\n",
    ")\n",
    "g.map(sns.barplot,\n",
    "      \"month\",\n",
    "      \"freq_var\",\n",
    "      orient='v',\n",
    "      order=INVERSE_MONTH_POS.keys(),\n",
    "      alpha=0.5)\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    ax.set_title(col_val)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeights_50best['stakes_new'] = dfWeights_50best['stakes'].apply(lambda x: rename_stakes[x]) \n",
    "dfWeights_50best['freq_var'] = dfWeights_50best['weight'] / 50\n",
    "dfWeights_50best_subset = dfWeights_50best[dfWeights_50best.stakes_new.apply(lambda x: 'Aletsch' in x)].sort_values(by = 'stakes_new')\n",
    "g = sns.FacetGrid(\n",
    "    dfWeights_50best_subset,\n",
    "    col=\"stakes_new\",\n",
    "    col_wrap=3,\n",
    "    hue=\"feature\",\n",
    ")\n",
    "g.map(sns.barplot,\n",
    "      \"month\",\n",
    "      \"freq_var\",\n",
    "      orient='v',\n",
    "      order=INVERSE_MONTH_POS.keys(),\n",
    "      alpha=0.5)\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    ax.set_title(col_val)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### 1 % highest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_combinations = 3249\n",
    "N_01best = math.ceil(N_combinations * 1 / 100)\n",
    "print('Number of 1% best combinations:', N_01best)\n",
    "feature_importdf_percbest, dfWeights_percbest = NBestCombinations(\n",
    "    dfAllStakes, INVERSE_MONTH_POS, t2m_vars, tp_vars, N=N_01best, type=VAL_METRIC)\n",
    "# Aggregate over all stakes:\n",
    "dfWeights_Mean_perc = dfWeights_percbest.groupby(['feature', 'month'\n",
    "                                                  ]).sum().reset_index()\n",
    "dfWeights_Mean_perc['type'] = np.tile(f'1% ({N_01best}) best',\n",
    "                                      len(dfWeights_Mean_perc))\n",
    "dfWeights_Mean_perc['freq_var'] = dfWeights_Mean_perc['weight'] / (N_01best *\n",
    "                                                                   30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeights_percbest['freq_var'] = dfWeights_percbest['weight'] / N_01best\n",
    "dfWeights_percbest['stakes_new'] = dfWeights_percbest['stakes'].apply(lambda x: rename_stakes[x]) \n",
    "dfWeights_percbest.sort_values(by='stakes', inplace=True)\n",
    "g = sns.FacetGrid(\n",
    "    dfWeights_percbest,\n",
    "    col=\"stakes_new\",\n",
    "    col_wrap=6,\n",
    "    hue=\"feature\",\n",
    ")\n",
    "g.map(sns.barplot,\n",
    "      \"month\",\n",
    "      \"freq_var\",\n",
    "      orient='v',\n",
    "      order=INVERSE_MONTH_POS.keys(),\n",
    "      alpha=0.5)\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeights_all = pd.concat([dfWeights_Mean_perc, dfWeights_Mean_50best], axis=0)\n",
    "g = sns.FacetGrid(\n",
    "    dfWeights_all,\n",
    "    col=\"feature\",\n",
    "    row='type',\n",
    "    height=2.5,\n",
    "    aspect=1.5,\n",
    ")\n",
    "g.map(sns.barplot,\n",
    "      \"month\",\n",
    "      \"freq_var\",\n",
    "      orient='v',\n",
    "      order=INVERSE_MONTH_POS.keys(),\n",
    "      color='#4d4d4d')\n",
    "colors = ['#e7e5f1', '#fef5e9']\n",
    "\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_ylabel('Frequency of month', fontsize = 15)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylim(top=1)\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    if col_val[0] == '1% (33) best':\n",
    "        ax.set_facecolor(colors[0])\n",
    "    if col_val[0] == '50 best':\n",
    "        ax.set_facecolor(colors[1])\n",
    "    ax.set_title('')\n",
    "    ax.grid()\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Best over stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take best average over all stakes:\n",
    "avgAllStakes = dfAllStakes.groupby('t2m-tp-hash').mean().sort_values(\n",
    "    by=VAL_METRIC)\n",
    "avgAllStakes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgAllStakes[VAL_METRIC].min(), avgAllStakes[VAL_METRIC].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importdf_1best, dfWeights_1best = NBestCombinations_avgStakes(\n",
    "    dfAllStakes, INVERSE_MONTH_POS, t2m_vars, tp_vars, N=1, type=VAL_METRIC)\n",
    "dfWeights_1best['freq_var'] = dfWeights_1best['weight'] / 1\n",
    "g = sns.FacetGrid(\n",
    "    dfWeights_1best,\n",
    "    col=\"feature\",\n",
    "    height=2.5,\n",
    "    aspect=1.5,\n",
    ")\n",
    "g.map(sns.barplot,\n",
    "      \"month\",\n",
    "      \"freq_var\",\n",
    "      orient='v',\n",
    "      order=INVERSE_MONTH_POS.keys(),\n",
    "      color='#4d4d4d')\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_ylabel('Frequency of month', fontsize = 15)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylim(top=1)\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### Fig 5b-e: Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights and feature importance over 50 and 1% best combinations:\n",
    "feature_importdf_50all, dfWeights_50all = NBestCombinations_avgStakes(\n",
    "    dfAllStakes, INVERSE_MONTH_POS, t2m_vars, tp_vars, N=50, type=VAL_METRIC)\n",
    "dfWeights_50all['freq_var'] = dfWeights_50all['weight'] / 50\n",
    "dfWeights_50all['type'] = np.tile('50 best', len(dfWeights_50all))\n",
    "\n",
    "N_01best = math.ceil(N_combinations * 1 / 100)\n",
    "feature_importdf_percall, dfWeights_percall = NBestCombinations_avgStakes(\n",
    "    dfAllStakes, INVERSE_MONTH_POS, t2m_vars, tp_vars, N=N_01best, type=VAL_METRIC)\n",
    "\n",
    "dfWeights_percall['type'] = np.tile(f'1% ({N_01best}) best',\n",
    "                                    len(dfWeights_percall))\n",
    "dfWeights_percall['freq_var'] = dfWeights_percall['weight'] / N_01best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeights_all = pd.concat([dfWeights_percall, dfWeights_50all], axis=0)\n",
    "g = sns.FacetGrid(\n",
    "    dfWeights_all,\n",
    "    col=\"feature\",\n",
    "    row='type',\n",
    "    height=2.5,\n",
    "    aspect=1.5,\n",
    ")\n",
    "g.map(sns.barplot,\n",
    "      \"month\",\n",
    "      \"freq_var\",\n",
    "      orient='v',\n",
    "      order=INVERSE_MONTH_POS.keys(),\n",
    "      color='#4d4d4d')\n",
    "colors = ['#e7e5f1', '#fef5e9']\n",
    "\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_ylabel('Frequency of month', fontsize = 15)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylim(top=1)\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    if col_val[0] == '1% (33) best':\n",
    "        val_mae = feature_importdf_percall[VAL_METRIC].unique()\n",
    "        ax.set_facecolor(colors[0])\n",
    "    if col_val[0] == '50 best':\n",
    "        val_mae = feature_importdf_50all[VAL_METRIC].unique()\n",
    "        ax.set_facecolor(colors[1])\n",
    "    ax.set_title('')\n",
    "    ax.grid()\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "#### Fig 5a: distribution of feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the MAE of putting the average measured PMB for each site: \n",
    "mae_mean = []\n",
    "for stake in var_xg_monthly['feat_test'].keys():\n",
    "    target = np.concatenate(var_xg_monthly['feat_test'][stake]['target_test'])\n",
    "    pred_mean = np.tile(np.mean(target), len(target))\n",
    "    mae_mean.append(mean_absolute_error(target, pred_mean))\n",
    "\n",
    "# average over all stakes:\n",
    "mae_mean = np.mean(mae_mean)/(1000)\n",
    "mae_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc, fifty = dfWeights_percall[VAL_METRIC].unique(), dfWeights_50all[VAL_METRIC].unique()\n",
    "perc, fifty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "perc, fifty = dfWeights_percall[VAL_METRIC].unique(), dfWeights_50all[VAL_METRIC].unique()\n",
    "sns.histplot(avgAllStakes, x=VAL_METRIC, kde=True, ax=ax, color = '#4d4d4d', alpha = 0.5)\n",
    "ax.set_xlabel('Validation MAE [m w.e.]')\n",
    "\n",
    "# colors = get_cmap_hex(cm.devon, 10)\n",
    "colors = ['#b2abd2', '#fee0b6']\n",
    "rect1 = matplotlib.patches.Rectangle((0,0), perc[0], 300, color=colors[0], alpha = 0.5)\n",
    "rect2 = matplotlib.patches.Rectangle((perc[0],0), fifty[0]-perc[0], 300, color=colors[1], alpha = 0.5)\n",
    "ax.add_patch(rect1)\n",
    "ax.add_patch(rect2)\n",
    "ax.axvline(x=mae_mean, color='grey', linestyle='--', label='Mean measured MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(dfAllStakes, col=\"stakes\", col_wrap=6)\n",
    "g.map(sns.kdeplot, VAL_METRIC, palette=color_palette, fill=True\n",
    "      #kde = True)\n",
    "      )\n",
    "for col_val, ax in g.axes_dict.items():\n",
    "    ax.set_xlabel(VAL_METRIC)\n",
    "    ax.set_ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Subset of best months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combi = [(['t2m_Apr','t2m_May', 't2m_June', 't2m_July', 't2m_Aug', 't2m_Sep'], [\n",
    "    'tp_Oct',\n",
    "    'tp_Nov',\n",
    "    'tp_Dec',\n",
    "    'tp_Jan',\n",
    "    'tp_Feb',\n",
    "])]\n",
    "best_months_t2m = [re.split('_', combi)[1] for combi in best_combi[0][0]]\n",
    "best_months_tp = [re.split('_', combi)[1] for combi in best_combi[0][1]]\n",
    "\n",
    "# Get all consecutive combinations of length max 6:\n",
    "def consecutive_combinations(iterable, consec):\n",
    "    begin = 0\n",
    "    chunks = len(iterable) + 1 - consec\n",
    "    return [iterable[x + begin:x + consec] for x in range(chunks)]\n",
    "\n",
    "\n",
    "iterable = list(best_months_t2m)\n",
    "consec_t2m = []\n",
    "for i in range(1, 7):\n",
    "    for el in consecutive_combinations(iterable, i):\n",
    "        consec_t2m.append(['t2m_' + j for j in el])\n",
    "consec_tp = ['tp_Oct', 'tp_Nov', 'tp_Dec', 'tp_Jan', 'tp_Feb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mae, vals_mae, hash = [], [], []\n",
    "for t2m_combi in consec_t2m:\n",
    "    t2mdf = dfAllStakes[dfAllStakes['t2m'].apply(lambda x: x == t2m_combi)]\n",
    "    val_mae.append(t2mdf[t2mdf['tp'].apply(lambda x: x == consec_tp)].groupby(\n",
    "        't2m-tp-hash').mean().val_mae.values[0])\n",
    "    vals_mae.append(\n",
    "        t2mdf[t2mdf['tp'].apply(lambda x: x == consec_tp)].val_mae.values)\n",
    "    hash.append(t2mdf[t2mdf['tp'].apply(lambda x: x == consec_tp)]\n",
    "                ['t2m-tp-hash'].values[0])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'val_mae': val_mae,\n",
    "    'vals_mae': vals_mae,\n",
    "    't2m_combi': consec_t2m,\n",
    "    'hash': hash\n",
    "}).sort_values(by='val_mae')\n",
    "\n",
    "df_expl = df[['hash', 'vals_mae']].explode('vals_mae')\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.boxplot(data=df_expl, x='hash', y='vals_mae', ax=ax, showmeans=True, order=df['hash'].values)\n",
    "\n",
    "t2mlabels = []\n",
    "for xlabel in ax.get_xticklabels():\n",
    "    hash = xlabel.get_text()\n",
    "    t2mlabels.append(df[df['hash'] == int(hash)].t2m_combi.values[0])\n",
    "\n",
    "ax.set_xticklabels(t2mlabels, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae, vals_mae, hash = [], [], []\n",
    "for t2m_combi in consec_t2m:\n",
    "    t2mdf = dfAllStakes[dfAllStakes['t2m'].apply(lambda x: x == t2m_combi)]\n",
    "    test_mae.append(t2mdf[t2mdf['tp'].apply(lambda x: x == consec_tp)].groupby(\n",
    "        't2m-tp-hash').mean().test_mae.values[0])\n",
    "    vals_mae.append(\n",
    "        t2mdf[t2mdf['tp'].apply(lambda x: x == consec_tp)].test_mae.values)\n",
    "    hash.append(t2mdf[t2mdf['tp'].apply(lambda x: x == consec_tp)]\n",
    "                ['t2m-tp-hash'].values[0])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'test_mae': test_mae,\n",
    "    'vals_mae': vals_mae,\n",
    "    't2m_combi': consec_t2m,\n",
    "    'hash': hash\n",
    "}).sort_values(by='test_mae')\n",
    "\n",
    "df_expl = df[['hash', 'vals_mae']].explode('vals_mae')\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.boxplot(data=df_expl, x='hash', y='vals_mae', ax=ax, showmeans=True,order=df['hash'].values)\n",
    "\n",
    "t2mlabels = []\n",
    "for xlabel in ax.get_xticklabels():\n",
    "    hash = xlabel.get_text()\n",
    "    t2mlabels.append(df[df['hash'] == int(hash)].t2m_combi.values[0])\n",
    "\n",
    "ax.set_xticklabels(t2mlabels, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Train miniML-MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyper parameters for training all combinations\n",
    "var_xg_monthly, metrics_monthly = assembleXGStakes(\n",
    "    path_save_xgboost_stakes +\n",
    "    f'{FOLD}/{INPUT_TYPE}/monthly/t2m_tp/match_annual/', glStakes_20years_all, rename_stakes, rename = False)\n",
    "hp_lr = metrics_monthly['hp_lr']\n",
    "hp_ne = metrics_monthly['hp_ne']\n",
    "hp_md = metrics_monthly['hp_md']\n",
    "\n",
    "hp_lr, hp_ne, hp_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### Normal miniML-MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combi = [(['t2m_May', 't2m_June', 't2m_July', 't2m_Aug'], [\n",
    "    'tp_Oct',\n",
    "    'tp_Nov',\n",
    "    'tp_Dec',\n",
    "    'tp_Jan',\n",
    "    'tp_Feb',\n",
    "])]\n",
    "\n",
    "best_months_t2m = [re.split('_', combi)[1] for combi in best_combi[0][0]]\n",
    "best_months_tp = [re.split('_', combi)[1] for combi in best_combi[0][1]]\n",
    "\n",
    "weights_t2m = np.ones(len(best_months_t2m))\n",
    "weights_tp = np.ones(len(best_months_tp))\n",
    "\n",
    "# Run XGBoost with best combinations of t2m variables\n",
    "RUN = True\n",
    "if RUN:\n",
    "    runXGBoost_one_varcomb(\n",
    "        best_combi,\n",
    "        hp_lr,\n",
    "        hp_ne,\n",
    "        hp_md,\n",
    "        glStakes_20years,\n",
    "        param_grid,  # grid for HP search\n",
    "        weights_t2m,\n",
    "        weights_tp,\n",
    "        mb_match='annual',\n",
    "        input_type=INPUT_TYPE,\n",
    "        log=False,\n",
    "        empty_folder=True,\n",
    "        grid_search=False,\n",
    "        input_vars={\n",
    "            \"t2m\": \"temperature\",\n",
    "            \"tp\": \"precipitation\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### miniML-MB with PDD instead of T:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combi = [(['t2m_May', 't2m_June', 't2m_July', 't2m_Aug'], [\n",
    "    'tp_Oct',\n",
    "    'tp_Nov',\n",
    "    'tp_Dec',\n",
    "    'tp_Jan',\n",
    "    'tp_Feb',\n",
    "])]\n",
    "\n",
    "best_months_t2m = [re.split('_', combi)[1] for combi in best_combi[0][0]]\n",
    "best_months_tp = [re.split('_', combi)[1] for combi in best_combi[0][1]]\n",
    "\n",
    "weights_t2m = np.ones(len(best_months_t2m))\n",
    "weights_tp = np.ones(len(best_months_tp))\n",
    "\n",
    "# Run XGBoost with PDD instead of t2m\n",
    "RUN = False\n",
    "if RUN:\n",
    "    runXGBoost_one_varcomb(\n",
    "        best_combi,\n",
    "        hp_lr,\n",
    "        hp_ne,\n",
    "        hp_md,\n",
    "        glStakes_20years,\n",
    "        param_grid,  # grid for HP search\n",
    "        weights_t2m,\n",
    "        weights_tp,\n",
    "        mb_match='annual',\n",
    "        input_type=INPUT_TYPE,\n",
    "        log=False,\n",
    "        empty_folder=True,\n",
    "        grid_search=False,\n",
    "        input_vars={\n",
    "            \"pdd\": \"temperature\",\n",
    "            \"tp\": \"precipitation\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "#### miniML-MB with weighted T and P:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combi = [(['t2m_May', 't2m_June', 't2m_July', 't2m_Aug'], [\n",
    "    'tp_Oct',\n",
    "    'tp_Nov',\n",
    "    'tp_Dec',\n",
    "    'tp_Jan',\n",
    "    'tp_Feb',\n",
    "])]\n",
    "\n",
    "best_months_t2m = [re.split('_', combi)[1] for combi in best_combi[0][0]]\n",
    "best_months_tp = [re.split('_', combi)[1] for combi in best_combi[0][1]]\n",
    "\n",
    "weights_all_t2m = dfWeights_all[(dfWeights_all.feature == 't2m')&(dfWeights_all.type == '50 best')]\n",
    "weights_all_tp = dfWeights_all[(dfWeights_all.feature == 'tp')&(dfWeights_all.type == '50 best')]\n",
    "\n",
    "weights_t2m = weights_all_t2m[weights_all_t2m.month.apply(lambda x: x in best_months_t2m)].freq_var.values\n",
    "weights_tp = weights_all_tp[weights_all_tp.month.apply(lambda x: x in best_months_tp)].freq_var.values\n",
    "\n",
    "weights_t2m, weights_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run XGBoost with best combinations of t2m variables\n",
    "RUN = False\n",
    "if RUN:\n",
    "    runXGBoost_one_varcomb(\n",
    "        best_combi,\n",
    "        hp_lr,\n",
    "        hp_ne,\n",
    "        hp_md,\n",
    "        glStakes_20years,\n",
    "        param_grid,  # grid for HP search\n",
    "        weights_t2m,\n",
    "        weights_tp,\n",
    "        mb_match='annual',\n",
    "        input_type=INPUT_TYPE,\n",
    "        log=False,\n",
    "        empty_folder=True,\n",
    "        grid_search=False,\n",
    "        input_vars={\n",
    "            \"t2m\": \"temperature\",\n",
    "            \"tp\": \"precipitation\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Clustering on stakes' combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = sns.color_palette(\"husl\", len(MONTH_VAL.keys()))\n",
    "palette = {}\n",
    "for ind in MONTH_VAL.keys():\n",
    "    palette[MONTH_VAL[ind]] = color_palette[ind - 1]\n",
    "\n",
    "feature_list = [\n",
    "    't2m_Oct', 't2m_Nov', 't2m_Dec', 't2m_Jan', 't2m_Feb', 't2m_Mar',\n",
    "    't2m_Apr', 't2m_May', 't2m_June', 't2m_July', 't2m_Aug', 't2m_Sep',\n",
    "    'tp_Oct', 'tp_Nov', 'tp_Dec', 'tp_Jan', 'tp_Feb', 'tp_Mar', 'tp_Apr',\n",
    "    'tp_May', 'tp_June', 'tp_July', 'tp_Aug', 'tp_Sep'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfWeights = dfWeights_percbest\n",
    "\n",
    "dfWeights = dfWeights_50best\n",
    "\n",
    "# Add a month key for clustering\n",
    "dfWeights['month_key'] = [\n",
    "    INVERSE_MONTH_POS[dfWeights.month.iloc[i]] for i in range(len(dfWeights))\n",
    "]\n",
    "dfWeights.sort_values(by=[\n",
    "    'stakes',\n",
    "    'feature',\n",
    "    'month_key',\n",
    "], inplace=True)\n",
    "\n",
    "# Transform dataframe for clustering so that there is one column per variable:\n",
    "df_cluster = pd.DataFrame()\n",
    "for stake in dfWeights.stakes.unique():\n",
    "    df_cluster = pd.concat([\n",
    "        df_cluster,\n",
    "        pd.DataFrame(dfWeights[dfWeights.stakes == stake].freq_var.values)\n",
    "    ],\n",
    "                           axis=1)\n",
    "df_cluster = pd.DataFrame()\n",
    "for stake in dfWeights.stakes.unique():\n",
    "    df_cluster = pd.concat([\n",
    "        df_cluster,\n",
    "        pd.DataFrame(dfWeights[dfWeights.stakes == stake].freq_var.values)\n",
    "    ],\n",
    "                           axis=1)\n",
    "df_cluster = df_cluster.transpose()\n",
    "df_cluster.columns = feature_list\n",
    "df_cluster['stake'] = dfWeights.stakes.unique()\n",
    "df_cluster.set_index('stake', inplace=True)\n",
    "df_cluster.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Clustering on TP & T2M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means++:\n",
    "# Here cluster on total temperature only\n",
    "X = df_cluster.values\n",
    "scl = StandardScaler()\n",
    "Xnorm = scl.fit_transform(X)\n",
    "# Kmeans params\n",
    "kmeans_params = {\n",
    "    'init': 'k-means++',\n",
    "    'max_iter': 300,\n",
    "    'n_init': 10,\n",
    "    'random_state': SEED\n",
    "}\n",
    "# Elbow method:\n",
    "model = KMeans(**kmeans_params)\n",
    "visualizer = KElbowVisualizer(model, k=(1, 11), timings=False)\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show()  # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Fig 11: Clustering of T & P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying K-Means to the dataset:\n",
    "N_c = 3  # number of clusters\n",
    "kmeans = KMeans(n_clusters=N_c, **kmeans_params)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "DF_cluster = df_cluster.copy()\n",
    "DF_cluster['cluster'] = y_kmeans\n",
    "\n",
    "# Assemble in a dataframe where columns are months (for plotting):\n",
    "# T2m:\n",
    "df_cluster_t2m = DF_cluster[[\n",
    "    't2m_Oct', 't2m_Nov', 't2m_Dec', 't2m_Jan', 't2m_Feb', 't2m_Mar',\n",
    "    't2m_Apr', 't2m_May', 't2m_June', 't2m_July', 't2m_Aug', 't2m_Sep',\n",
    "    'cluster'\n",
    "]]\n",
    "df_cluster_t2m.rename(columns={\n",
    "    't2m_Oct': 0,\n",
    "    't2m_Nov': 1,\n",
    "    't2m_Dec': 2,\n",
    "    't2m_Jan': 3,\n",
    "    't2m_Feb': 4,\n",
    "    't2m_Mar': 5,\n",
    "    't2m_Apr': 6,\n",
    "    't2m_May': 7,\n",
    "    't2m_June': 8,\n",
    "    't2m_July': 9,\n",
    "    't2m_Aug': 10,\n",
    "    't2m_Sep': 11\n",
    "},inplace=True)\n",
    "df_cluster_t2m['feature'] = np.tile('t2m', len(df_cluster_t2m))\n",
    "\n",
    "# TP:\n",
    "df_cluster_tp = DF_cluster[[\n",
    "    'tp_Oct', 'tp_Nov', 'tp_Dec', 'tp_Jan', 'tp_Feb', 'tp_Mar', 'tp_Apr',\n",
    "    'tp_May', 'tp_June', 'tp_July', 'tp_Aug', 'tp_Sep', 'cluster'\n",
    "]]\n",
    "df_cluster_tp.rename(columns={\n",
    "    'tp_Oct': 0,\n",
    "    'tp_Nov': 1,\n",
    "    'tp_Dec': 2,\n",
    "    'tp_Jan': 3,\n",
    "    'tp_Feb': 4,\n",
    "    'tp_Mar': 5,\n",
    "    'tp_Apr': 6,\n",
    "    'tp_May': 7,\n",
    "    'tp_June': 8,\n",
    "    'tp_July': 9,\n",
    "    'tp_Aug': 10,\n",
    "    'tp_Sep': 11\n",
    "},inplace=True)\n",
    "df_cluster_tp['feature'] = np.tile('tp', len(df_cluster_tp))\n",
    "df_clusters_per_feat = pd.concat([df_cluster_t2m, df_cluster_tp], axis=0)\n",
    "\n",
    "PlotFeatClusters(df_clusters_per_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attributes of clusters:\n",
    "cl_elev, clnb, cl_lat, cl_lon, stakes, glaciers, glshort = [], [], [], [], [], [], []\n",
    "training_mb, training_y, len_training = [], [], []\n",
    "for cl_nb in DF_cluster['cluster'].unique():\n",
    "    cl_stakes = DF_cluster[DF_cluster['cluster'] == cl_nb].index\n",
    "    for stake in cl_stakes:\n",
    "        f_stake = read_stake_csv(path_glacattr, f'{stake}_mb.csv')\n",
    "        cl_elev.append(np.mean(f_stake.height))\n",
    "        cl_lat.append(np.mean(f_stake.lat))\n",
    "        cl_lon.append(np.mean(f_stake.lon))\n",
    "        len_training.append(\n",
    "            len(var_xg_monthly['feat_train'][stake]['target']) / NUM_FOLDS)\n",
    "        # mean training mb\n",
    "        training_mb.append(\n",
    "            np.mean(var_xg_monthly['feat_train'][stake]['target']))\n",
    "        training_y.append(\n",
    "            int(np.mean(var_xg_monthly['feat_train'][stake]['time'])))\n",
    "        clnb.append(cl_nb+1)\n",
    "        stakes.append(stake)\n",
    "        glaciers.append(re.split('_', stake)[0])\n",
    "        glshort.append(GL_SHORT[re.split('_', stake)[0].title()] + '_' +\n",
    "                       re.split('_', stake)[1])\n",
    "\n",
    "df_info = pd.DataFrame({\n",
    "    'elevation': cl_elev,\n",
    "    'lon': cl_lon,\n",
    "    'lat': cl_lat,\n",
    "    'training_mb': training_mb,\n",
    "    'training_time': training_y,\n",
    "    'training_length': len_training,\n",
    "    'cluster': clnb,\n",
    "    'stakes': stakes,\n",
    "    'glaciers': glaciers,\n",
    "    'glshort': glshort\n",
    "})\n",
    "df_info[\"cluster\"] = df_info[\"cluster\"].astype(\"category\")\n",
    "\n",
    "mean_df = pd.DataFrame({\n",
    "    'cluster':\n",
    "    range(1, N_c+1),\n",
    "    'mean_el':\n",
    "    df_info.groupby('cluster').mean().elevation,\n",
    "    'mean_mb':\n",
    "    df_info.groupby('cluster').mean()['training_mb']\n",
    "})\n",
    "for clusterNb in range(df_info['cluster'].nunique()):\n",
    "    stakes_c0 = df_info[df_info['cluster'] == clusterNb].stakes.values\n",
    "    print(f'Stakes of cluster {clusterNb}:\\n {stakes_c0}')\n",
    "plotClusterStats(df_info, mean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Clustering on TP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means++:\n",
    "# Here cluster on total precipitation only\n",
    "X = df_cluster[[\n",
    "    'tp_Oct', 'tp_Nov', 'tp_Dec', 'tp_Jan', 'tp_Feb', 'tp_Mar', 'tp_Apr',\n",
    "    'tp_May', 'tp_June', 'tp_July', 'tp_Aug', 'tp_Sep'\n",
    "]].values\n",
    "scl = StandardScaler()\n",
    "Xnorm = scl.fit_transform(X)\n",
    "# Kmeans params\n",
    "kmeans_params = {\n",
    "    'init': 'k-means++',\n",
    "    'max_iter': 300,\n",
    "    'n_init': 10,\n",
    "    'random_state': SEED\n",
    "}\n",
    "# Elbow method:\n",
    "model = KMeans(**kmeans_params)\n",
    "visualizer = KElbowVisualizer(model, k=(1, 11))\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show()  # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying K-Means to the dataset:\n",
    "N_c = 3  # number of clusters\n",
    "kmeans = KMeans(n_clusters=N_c, **kmeans_params)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "DF_cluster = df_cluster.copy()\n",
    "DF_cluster['cluster'] = y_kmeans\n",
    "\n",
    "# Assemble in a dataframe where columns are months (for plotting):\n",
    "# T2m:\n",
    "df_cluster_t2m = DF_cluster[[\n",
    "    't2m_Oct', 't2m_Nov', 't2m_Dec', 't2m_Jan', 't2m_Feb', 't2m_Mar',\n",
    "    't2m_Apr', 't2m_May', 't2m_June', 't2m_July', 't2m_Aug', 't2m_Sep',\n",
    "    'cluster'\n",
    "]]\n",
    "df_cluster_t2m.rename(columns={\n",
    "    't2m_Oct': 0,\n",
    "    't2m_Nov': 1,\n",
    "    't2m_Dec': 2,\n",
    "    't2m_Jan': 3,\n",
    "    't2m_Feb': 4,\n",
    "    't2m_Mar': 5,\n",
    "    't2m_Apr': 6,\n",
    "    't2m_May': 7,\n",
    "    't2m_June': 8,\n",
    "    't2m_July': 9,\n",
    "    't2m_Aug': 10,\n",
    "    't2m_Sep': 11\n",
    "},inplace=True)\n",
    "df_cluster_t2m['feature'] = np.tile('t2m', len(df_cluster_t2m))\n",
    "\n",
    "# TP:\n",
    "df_cluster_tp = DF_cluster[[\n",
    "    'tp_Oct', 'tp_Nov', 'tp_Dec', 'tp_Jan', 'tp_Feb', 'tp_Mar', 'tp_Apr',\n",
    "    'tp_May', 'tp_June', 'tp_July', 'tp_Aug', 'tp_Sep', 'cluster'\n",
    "]]\n",
    "df_cluster_tp.rename(columns={\n",
    "    'tp_Oct': 0,\n",
    "    'tp_Nov': 1,\n",
    "    'tp_Dec': 2,\n",
    "    'tp_Jan': 3,\n",
    "    'tp_Feb': 4,\n",
    "    'tp_Mar': 5,\n",
    "    'tp_Apr': 6,\n",
    "    'tp_May': 7,\n",
    "    'tp_June': 8,\n",
    "    'tp_July': 9,\n",
    "    'tp_Aug': 10,\n",
    "    'tp_Sep': 11\n",
    "},inplace=True)\n",
    "df_cluster_tp['feature'] = np.tile('tp', len(df_cluster_tp))\n",
    "df_clusters_per_feat = pd.concat([df_cluster_t2m, df_cluster_tp], axis=0)\n",
    "df_clusters_per_feat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotFeatClusters(df_clusters_per_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "#### Attributes per cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "GL_SHORT = {'basodino': 'BAS',\n",
    " 'gries': 'GRI',\n",
    " 'schwarzberg': 'SCH',\n",
    " 'aletsch': 'ALE',\n",
    " 'limmern': 'LIM',\n",
    " 'clariden': 'CLA',\n",
    " 'allalin': 'ALL',\n",
    " 'silvretta': 'SIL',\n",
    " 'hohlaub': 'HOH',\n",
    " 'pers': 'PERS',\n",
    " 'corbassiere': 'COR',\n",
    " 'plattalva': 'PLA',\n",
    " 'gietro': 'GIE'}\n",
    "\n",
    "# Get attributes of clusters:\n",
    "cl_elev, clnb, cl_lat, cl_lon, stakes, glaciers, glshort = [], [], [], [], [], [], []\n",
    "training_mb, training_y, len_training = [], [], []\n",
    "for cl_nb in DF_cluster['cluster'].unique():\n",
    "    cl_stakes = DF_cluster[DF_cluster['cluster'] == cl_nb].index\n",
    "    for stake in cl_stakes:\n",
    "        f_stake = read_stake_csv(path_glacattr, f'{stake}_mb.csv')\n",
    "        cl_elev.append(np.mean(f_stake.height))\n",
    "        cl_lat.append(np.mean(f_stake.lat))\n",
    "        cl_lon.append(np.mean(f_stake.lon))\n",
    "        len_training.append(\n",
    "            len(var_xg_monthly['feat_train'][stake]['target']) / NUM_FOLDS)\n",
    "        # mean training mb\n",
    "        training_mb.append(\n",
    "            np.mean(var_xg_monthly['feat_train'][stake]['target']))\n",
    "        training_y.append(\n",
    "            int(np.mean(var_xg_monthly['feat_train'][stake]['time'])))\n",
    "        clnb.append(cl_nb)\n",
    "        stakes.append(stake)\n",
    "        glaciers.append(re.split('_', stake)[0])\n",
    "        glshort.append(GL_SHORT[re.split('_', stake)[0]] + '_' +\n",
    "                       re.split('_', stake)[1])\n",
    "\n",
    "df_info = pd.DataFrame({\n",
    "    'elevation': cl_elev,\n",
    "    'lon': cl_lon,\n",
    "    'lat': cl_lat,\n",
    "    'training_mb': training_mb,\n",
    "    'training_time': training_y,\n",
    "    'training_length': len_training,\n",
    "    'cluster': clnb,\n",
    "    'stakes': stakes,\n",
    "    'glaciers': glaciers,\n",
    "    'glshort': glshort\n",
    "})\n",
    "df_info[\"cluster\"] = df_info[\"cluster\"].astype(\"category\")\n",
    "\n",
    "mean_df = pd.DataFrame({\n",
    "    'cluster':\n",
    "    range(0, N_c),\n",
    "    'mean_el':\n",
    "    df_info.groupby('cluster').mean().elevation,\n",
    "    'mean_mb':\n",
    "    df_info.groupby('cluster').mean()['training_mb']\n",
    "})\n",
    "stakes_c0 = df_info[df_info['cluster'] == 0].stakes.values\n",
    "stakes_c1 = df_info[df_info['cluster'] == 1].stakes.values\n",
    "stakes_c2 = df_info[df_info['cluster'] == 2].stakes.values\n",
    "print(f'Stakes of cluster 0:\\n {stakes_c0}')\n",
    "print(f'Stakes of cluster 1:\\n {stakes_c1}')\n",
    "print(f'Stakes of cluster 2:\\n {stakes_c2}')\n",
    "\n",
    "plotClusterStats(df_info, mean_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Clustering on T2M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means++:\n",
    "# Here cluster on total temperature only\n",
    "X = df_cluster[[\n",
    "    't2m_Oct', 't2m_Nov', 't2m_Dec', 't2m_Jan', 't2m_Feb', 't2m_Mar',\n",
    "    't2m_Apr', 't2m_May', 't2m_June', 't2m_July', 't2m_Aug', 't2m_Sep',\n",
    "]].values\n",
    "scl = StandardScaler()\n",
    "Xnorm = scl.fit_transform(X)\n",
    "# Kmeans params\n",
    "kmeans_params = {\n",
    "    'init': 'k-means++',\n",
    "    'max_iter': 300,\n",
    "    'n_init': 10,\n",
    "    'random_state': SEED\n",
    "}\n",
    "# Elbow method:\n",
    "model = KMeans(**kmeans_params)\n",
    "visualizer = KElbowVisualizer(model, k=(1, 11))\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show()  # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying K-Means to the dataset:\n",
    "N_c = 3  # number of clusters\n",
    "kmeans = KMeans(n_clusters=N_c, **kmeans_params)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "DF_cluster = df_cluster.copy()\n",
    "DF_cluster['cluster'] = y_kmeans\n",
    "# Assemble in a dataframe where columns are months (for plotting):\n",
    "# T2m:\n",
    "df_cluster_t2m = DF_cluster[[\n",
    "    't2m_Oct', 't2m_Nov', 't2m_Dec', 't2m_Jan', 't2m_Feb', 't2m_Mar',\n",
    "    't2m_Apr', 't2m_May', 't2m_June', 't2m_July', 't2m_Aug', 't2m_Sep',\n",
    "    'cluster'\n",
    "]]\n",
    "df_cluster_t2m.rename(columns={\n",
    "    't2m_Oct': 0,\n",
    "    't2m_Nov': 1,\n",
    "    't2m_Dec': 2,\n",
    "    't2m_Jan': 3,\n",
    "    't2m_Feb': 4,\n",
    "    't2m_Mar': 5,\n",
    "    't2m_Apr': 6,\n",
    "    't2m_May': 7,\n",
    "    't2m_June': 8,\n",
    "    't2m_July': 9,\n",
    "    't2m_Aug': 10,\n",
    "    't2m_Sep': 11\n",
    "},inplace=True)\n",
    "df_cluster_t2m['feature'] = np.tile('t2m', len(df_cluster_t2m))\n",
    "\n",
    "# TP:\n",
    "df_cluster_tp = DF_cluster[[\n",
    "    'tp_Oct', 'tp_Nov', 'tp_Dec', 'tp_Jan', 'tp_Feb', 'tp_Mar', 'tp_Apr',\n",
    "    'tp_May', 'tp_June', 'tp_July', 'tp_Aug', 'tp_Sep', 'cluster'\n",
    "]]\n",
    "df_cluster_tp.rename(columns={\n",
    "    'tp_Oct': 0,\n",
    "    'tp_Nov': 1,\n",
    "    'tp_Dec': 2,\n",
    "    'tp_Jan': 3,\n",
    "    'tp_Feb': 4,\n",
    "    'tp_Mar': 5,\n",
    "    'tp_Apr': 6,\n",
    "    'tp_May': 7,\n",
    "    'tp_June': 8,\n",
    "    'tp_July': 9,\n",
    "    'tp_Aug': 10,\n",
    "    'tp_Sep': 11\n",
    "},inplace=True)\n",
    "df_cluster_tp['feature'] = np.tile('tp', len(df_cluster_tp))\n",
    "df_clusters_per_feat = pd.concat([df_cluster_t2m, df_cluster_tp], axis=0)\n",
    "df_clusters_per_feat.head(2)\n",
    "\n",
    "PlotFeatClusters(df_clusters_per_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attributes of clusters:\n",
    "cl_elev, clnb, cl_lat, cl_lon, stakes, glaciers, glshort = [], [], [], [], [], [], []\n",
    "training_mb, training_y, len_training = [], [], []\n",
    "for cl_nb in DF_cluster['cluster'].unique():\n",
    "    cl_stakes = DF_cluster[DF_cluster['cluster'] == cl_nb].index\n",
    "    for stake in cl_stakes:\n",
    "        f_stake = read_stake_csv(path_glacattr, f'{stake}_mb.csv')\n",
    "        cl_elev.append(np.mean(f_stake.height))\n",
    "        cl_lat.append(np.mean(f_stake.lat))\n",
    "        cl_lon.append(np.mean(f_stake.lon))\n",
    "        len_training.append(\n",
    "            len(var_xg_monthly['feat_train'][stake]['target']) / NUM_FOLDS)\n",
    "        # mean training mb\n",
    "        training_mb.append(\n",
    "            np.mean(var_xg_monthly['feat_train'][stake]['target']))\n",
    "        training_y.append(\n",
    "            int(np.mean(var_xg_monthly['feat_train'][stake]['time'])))\n",
    "        clnb.append(cl_nb)\n",
    "        stakes.append(stake)\n",
    "        glaciers.append(re.split('_', stake)[0])\n",
    "        glshort.append(GL_SHORT[re.split('_', stake)[0]] + '_' +\n",
    "                       re.split('_', stake)[1])\n",
    "\n",
    "df_info = pd.DataFrame({\n",
    "    'elevation': cl_elev,\n",
    "    'lon': cl_lon,\n",
    "    'lat': cl_lat,\n",
    "    'training_mb': training_mb,\n",
    "    'training_time': training_y,\n",
    "    'training_length': len_training,\n",
    "    'cluster': clnb,\n",
    "    'stakes': stakes,\n",
    "    'glaciers': glaciers,\n",
    "    'glshort': glshort\n",
    "})\n",
    "df_info[\"cluster\"] = df_info[\"cluster\"].astype(\"category\")\n",
    "\n",
    "mean_df = pd.DataFrame({\n",
    "    'cluster':\n",
    "    range(0, N_c),\n",
    "    'mean_el':\n",
    "    df_info.groupby('cluster').mean().elevation,\n",
    "    'mean_mb':\n",
    "    df_info.groupby('cluster').mean()['training_mb']\n",
    "})\n",
    "stakes_c0 = df_info[df_info['cluster'] == 0].stakes.values\n",
    "stakes_c1 = df_info[df_info['cluster'] == 1].stakes.values\n",
    "stakes_c2 = df_info[df_info['cluster'] == 2].stakes.values\n",
    "print(f'Stakes of cluster 0:\\n {stakes_c0}')\n",
    "print(f'Stakes of cluster 1:\\n {stakes_c1}')\n",
    "print(f'Stakes of cluster 2:\\n {stakes_c2}')\n",
    "\n",
    "plotClusterStats(df_info, mean_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
